import express from "express";
import cors from "cors";
import { ChatVertexAI } from "@langchain/google-vertexai";
import { BaseLanguageModelInput } from "@langchain/core/language_models/base";
import { VertexAIEmbeddings } from "@langchain/google-vertexai";
import { MongoDBAtlasVectorSearch } from "@langchain/mongodb";

import { config } from "./config.js";
import { connectToDatabase, collections } from "./database.js";

const app = express();
app.use(cors());

const router = express.Router();
router.use(express.json());

router.get("/", async (_, res) => {
  res.send("Bem-vindo Ã  API do Chatbot de Seguros! ðŸ§");
});
// Inicializar o modelo conversacional do Vertex AI
const model = new ChatVertexAI({
    // Usaremos o modelo Gemini 2.0 Flash
    model: "gemini-2.0-flash",
    // NÃºmero mÃ¡ximo de tokens a serem gerados na resposta
    maxOutputTokens: 2048,
    // O parÃ¢metro "temperature" controla a aleatoriedade da resposta â€” quanto maior o valor, mais aleatÃ³ria serÃ¡ a saÃ­da
    temperature: 0.5,
    // O parÃ¢metro "topP" controla a diversidade da resposta â€” quanto maior o valor, mais diversa serÃ¡ a saÃ­da
    topP: 0.9,
    // O parÃ¢metro "topK" tambÃ©m controla a diversidade da resposta â€” quanto maior o valor, mais diversa serÃ¡ a saÃ­da
    topK: 20,
});

// Armazenar o histÃ³rico de chat, comeÃ§ando com a mensagem do sistema informando que o assistente Ã© especializado em apÃ³lices de seguro
const history: BaseLanguageModelInput = [
    [
        "system",
        `VocÃª Ã© um assistente experiente e confiÃ¡vel em apÃ³lices de seguro. ForneÃ§a apenas informaÃ§Ãµes precisas e verificadas relacionadas a apÃ³lices de seguro. NÃ£o responda a perguntas irrelevantes ou sem sentido.

Utilize qualquer contexto fornecido sobre as apÃ³lices de seguro do usuÃ¡rio, como detalhes da cobertura, termos da apÃ³lice e procedimentos de solicitaÃ§Ã£o de indenizaÃ§Ã£o, para garantir que suas respostas sejam precisas e pertinentes. NÃ£o mencione que o contexto foi utilizado para gerar a resposta. Inclua apenas informaÃ§Ãµes diretamente relevantes Ã  consulta do usuÃ¡rio.`
    ],
];

router.post("/messages", async (req, res) => {
    let message = req.body.text;
    if (!message) {
        return res.status(400).send({ error: 'Mensagem Ã© obrigatÃ³ria' });
    }

    let prompt = `Pergunta do usuÃ¡rio: ${message}.`;

    try {
        const modelResponse = await model.invoke([...history, prompt]);
        const textResponse = modelResponse?.content;

        if (!textResponse) {
            return res.status(500).send({ error: 'Falha na invocaÃ§Ã£o do modelo.' });
        }

        history.push([
            "humano",
            message
        ]);

        history.push([
            "assistente",
            textResponse
        ]);

        return res.send({ text: textResponse });
    } catch (e) {
        console.error(e);
        return res.status(500).send({ error: 'Falha na invocaÃ§Ã£o do modelo.' });
    }
});



app.use(router);

// start the Express server
app.listen(config.server.port, () => {
  console.log(`Server running on port:${config.server.port}...`);
});
