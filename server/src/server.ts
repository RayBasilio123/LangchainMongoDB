import express from "express";
import cors from "cors";
import { ChatVertexAI } from "@langchain/google-vertexai";
import { BaseLanguageModelInput } from "@langchain/core/language_models/base";
import { VertexAIEmbeddings } from "@langchain/google-vertexai";
import { MongoDBAtlasVectorSearch } from "@langchain/mongodb";

import { config } from "./config.js";
import { connectToDatabase, collections } from "./database.js";

const app = express();
app.use(cors());

const router = express.Router();
router.use(express.json());

router.get("/", async (_, res) => {
  res.send("Bem-vindo Ã  API do Chatbot de Seguros! ðŸ§");
});
// Inicializar o modelo conversacional do Vertex AI
const model = new ChatVertexAI({
    // Usaremos o modelo Gemini 2.0 Flash
    model: "gemini-2.0-flash",
    // NÃºmero mÃ¡ximo de tokens a serem gerados na resposta
    maxOutputTokens: 2048,
    // O parÃ¢metro "temperature" controla a aleatoriedade da resposta â€” quanto maior o valor, mais aleatÃ³ria serÃ¡ a saÃ­da
    temperature: 0.5,
    // O parÃ¢metro "topP" controla a diversidade da resposta â€” quanto maior o valor, mais diversa serÃ¡ a saÃ­da
    topP: 0.9,
    // O parÃ¢metro "topK" tambÃ©m controla a diversidade da resposta â€” quanto maior o valor, mais diversa serÃ¡ a saÃ­da
    topK: 20,
});

// Armazenar o histÃ³rico de chat, comeÃ§ando com a mensagem do sistema informando que o assistente Ã© especializado em apÃ³lices de seguro
const history: BaseLanguageModelInput = [
    [
        "system",
        `VocÃª Ã© um assistente experiente e confiÃ¡vel em apÃ³lices de seguro. ForneÃ§a apenas informaÃ§Ãµes precisas e verificadas relacionadas a apÃ³lices de seguro. NÃ£o responda a perguntas irrelevantes ou sem sentido.

Utilize qualquer contexto fornecido sobre as apÃ³lices de seguro do usuÃ¡rio, como detalhes da cobertura, termos da apÃ³lice e procedimentos de solicitaÃ§Ã£o de indenizaÃ§Ã£o, para garantir que suas respostas sejam precisas e pertinentes. NÃ£o mencione que o contexto foi utilizado para gerar a resposta. Inclua apenas informaÃ§Ãµes diretamente relevantes Ã  consulta do usuÃ¡rio.`
    ],
];
// Conectar ao banco de dados MongoDB Atlas
await connectToDatabase();

// Inicializar o armazenamento vetorial do MongoDB Atlas com a configuraÃ§Ã£o especificada
const vectorStore = new MongoDBAtlasVectorSearch(
  // O modelo de embeddings de texto do Google Cloud Vertex AI serÃ¡ usado para vetorizar os blocos de texto
  new VertexAIEmbeddings({
    model: "text-embedding-005"
  }),
  {
    collection: collections.context as any,
    // Nome do Ã­ndice de busca vetorial no Atlas. VocÃª deve criÃ¡-lo na interface do Atlas.
    indexName: "vector_index",
    // Nome do campo da coleÃ§Ã£o que contÃ©m o conteÃºdo original. O padrÃ£o Ã© "text"
    textKey: "text",
    // Nome do campo da coleÃ§Ã£o que contÃ©m os textos vetorizados (embeddings). O padrÃ£o Ã© "embedding"
    embeddingKey: "embedding",
  }
);

// Inicializar um "retriever" (buscador) com base no armazenamento vetorial do MongoDB Atlas
const vectorStoreRetriever = vectorStore.asRetriever();

router.post("/messages", async (req, res) => {
    let message = req.body.text;
    if (!message) {
        return res.status(400).send({ error: 'Mensagem Ã© obrigatÃ³ria' });
    }

    let prompt = `Pergunta do usuÃ¡rio: ${message}.`;
    // Se o modo RAG estiver ativado, recuperar o contexto a partir do armazenamento vetorial do MongoDB Atlas
    const rag = req.body.rag;
    if (rag) {
        const context = await vectorStoreRetriever.invoke(message);

        if (context) {
            prompt += `

            Contexto:
            ${context?.map(doc => doc.pageContent).join("\n")}
            `;
        } else {
            console.error("Falha na recuperaÃ§Ã£o do contexto");
        }
    }

    try {
        const modelResponse = await model.invoke([...history, prompt]);
        const textResponse = modelResponse?.content;

        if (!textResponse) {
            return res.status(500).send({ error: 'Falha na invocaÃ§Ã£o do modelo.' });
        }

        history.push([
            "humano",
            message
        ]);

        history.push([
            "assistente",
            textResponse
        ]);

        return res.send({ text: textResponse });
    } catch (e) {
        console.error(e);
        return res.status(500).send({ error: 'Falha na invocaÃ§Ã£o do modelo.' });
    }
});



app.use(router);

// start the Express server
app.listen(config.server.port, () => {
  console.log(`Server running on port:${config.server.port}...`);
});
